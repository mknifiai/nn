{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27b62236-3d50-41ae-9a1a-1a13befa7572",
   "metadata": {},
   "source": [
    "# The Perceptron Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78e368c-81ca-4dcd-a87b-27d25db9713c",
   "metadata": {},
   "source": [
    "__The network consists of:__\n",
    "- input nodes, i.e., the dimension (number of features) in the input vector\n",
    "- The neurons\n",
    "    - $w_{ij}$, where the $i$ runs over the number of features and $j$ runs over the number of neurons.\n",
    "    - So $w_{32}$ is the weight that connects input node 3 to neuron 2\n",
    "    - activation of each neuron $j$ using activation function $g$. $  $  $g = \\Sigma_{i=0}^{m}(w_{ij}x_{i})$\n",
    "    - Loss function $\\frac{1}{2}(y_{j}-t_{j})^{2}$, where $y = w \\cdot x $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f63ff9-eb82-4c16-b246-91d97e0eed2a",
   "metadata": {},
   "source": [
    "__How learning occurs?__\n",
    "- We use rule for updating a weight $w_{ij}$ as follows: $w_{ij} = w_{ij} −η(y_{j} −t_{j})·x_{i}$, <br>\n",
    "- where $(y_{j}−t_{j})·x_{i}$ is the gradient of the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b550fca-5945-44b9-adaa-c65b05f78d45",
   "metadata": {},
   "source": [
    "<img src=\"perceptron.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be3d7f4-4832-433c-adaf-262c5da09782",
   "metadata": {},
   "source": [
    "#### What is the Bias Input?\n",
    "- To deal with the case where all of the inputs to a neuron are zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9bd6b4-2d24-4b6e-a156-ede4c9dfbe6e",
   "metadata": {},
   "source": [
    "# The Perceptron Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0925e6eb-d348-4d5c-9f38-001f5cad68db",
   "metadata": {},
   "source": [
    "<img src=\"pAlgorithm.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff75a83-2a9e-455f-a5c2-b447daeb1d66",
   "metadata": {},
   "source": [
    "#### The Learning Rate η\n",
    "- the value of the learning rate decides how fast the network learns\n",
    "- the parameter η controlling how much to change the weights by. \n",
    "- setting it to 1, the weights change a lot whenever there is a wrong answer, which tends to make the network unstable, so that it never settles down.\n",
    "- The cost of having a small learning rate is that the weights need to see the inputs more often before they change significantly, so that the network takes longer to learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16e078a-cb4c-464b-bda2-cff440695377",
   "metadata": {},
   "source": [
    "## An Example of Perceptron Learning: Logic Functions (the logical OR)\n",
    "### Linear Separability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f2ce44-dbbc-42b7-9453-db2406e467ba",
   "metadata": {},
   "source": [
    "<img src=\"or.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fb0df7-8fe6-43f7-afa5-48823649f95f",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "90768dc8-8f27-4a49-ade3-acb1f02b3acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Make some training data\n",
    "\n",
    "# OR Function\n",
    "#a = np.array([[0,0,0],[0,1,0],[1,0,0],[1,1,1]])\n",
    "\n",
    "# XOR Function\n",
    "a = np.array([[0,0,0],[0,1,1],[1,0,1],[1,1,0]])\n",
    "\n",
    "X = a[:,0:2]\n",
    "T = a[:,2:]\n",
    "\n",
    "nSamples = X.shape[0]\n",
    "nOutputs = T.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "515bb342-d267-4244-8bba-87b427b17182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7b6658c1-9912-4ba2-9a37-672c2386b823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy =  0.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set parameters of neural network\n",
    "eta = 0.25\n",
    "\n",
    "# Initialize weights to uniformly distributed values between small normally-distributed between -0.1 and 0.1\n",
    "W = 0.1 * 2 * (np.random.uniform(size=(1 + X.shape[1], nOutputs)) - 0.5)\n",
    "\n",
    "# Add constant column of 1's\n",
    "def addOnes(A):\n",
    "    return np.insert(A, 0, 1, axis=1)\n",
    "\n",
    "X1 = addOnes(X)\n",
    "\n",
    "\n",
    "# Take nSteps steepest descent steps in gradient descent search\n",
    "nSteps = 30\n",
    "\n",
    "for step in range(nSteps):\n",
    "\n",
    "    # Forward pass on training data\n",
    "    Y = X1 @ W \n",
    "    Y = np.where(Y>0,1,0)\n",
    "\n",
    "    # Error in output\n",
    "    error = T - Y\n",
    "\n",
    "    W = W + eta * X1.T @ error\n",
    "    \n",
    "Ytest = np.where((X1 @ W) > 0,1,0)  #!! Forward pass in one line\n",
    "\n",
    "print('Accuracy = ', (sum(Ytest==T)/nSamples)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fcd2cd-bc96-4d39-aea1-9963c7af8935",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40d1f144-67fd-4917-8c1a-3c08cd6c15bd",
   "metadata": {},
   "source": [
    "## A decision boundary separating two classes of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56ea695-f54f-4289-8cad-1501c6c5e7b5",
   "metadata": {},
   "source": [
    "<img src=\"bp.png\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d25d557-298c-439e-aa91-286a132f67c5",
   "metadata": {},
   "source": [
    "## Different decision boundaries computed by a Perceptron with four neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f896fa-465d-4e72-8513-0acb6a7bc82e",
   "metadata": {},
   "source": [
    "<img src=\"mcp.png\" width=400 height=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da71b4c-0457-400e-bd51-a749cab6ae83",
   "metadata": {},
   "source": [
    "### The Exclusive Or (XOR) Function (non-linear function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1592e73c-5417-4faa-998a-35399371e460",
   "metadata": {},
   "source": [
    "<img src=\"xor.png\" width=500 height=500 />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
